<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />
<title>Fourier Neural Operators</title>
<style type="text/css">

/*
:Author: David Goodger (goodger@python.org)
:Id: $Id: html4css1.css 8954 2022-01-20 10:10:25Z milde $
:Copyright: This stylesheet has been placed in the public domain.

Default cascading style sheet for the HTML output of Docutils.

See https://docutils.sourceforge.io/docs/howto/html-stylesheets.html for how to
customize this style sheet.
*/

/* used to remove borders from tables and images */
.borderless, table.borderless td, table.borderless th {
  border: 0 }

table.borderless td, table.borderless th {
  /* Override padding for "table.docutils td" with "! important".
     The right padding separates the table cells. */
  padding: 0 0.5em 0 0 ! important }

.first {
  /* Override more specific margin styles with "! important". */
  margin-top: 0 ! important }

.last, .with-subtitle {
  margin-bottom: 0 ! important }

.hidden {
  display: none }

.subscript {
  vertical-align: sub;
  font-size: smaller }

.superscript {
  vertical-align: super;
  font-size: smaller }

a.toc-backref {
  text-decoration: none ;
  color: black }

blockquote.epigraph {
  margin: 2em 5em ; }

dl.docutils dd {
  margin-bottom: 0.5em }

object[type="image/svg+xml"], object[type="application/x-shockwave-flash"] {
  overflow: hidden;
}

/* Uncomment (and remove this text!) to get bold-faced definition list terms
dl.docutils dt {
  font-weight: bold }
*/

div.abstract {
  margin: 2em 5em }

div.abstract p.topic-title {
  font-weight: bold ;
  text-align: center }

div.admonition, div.attention, div.caution, div.danger, div.error,
div.hint, div.important, div.note, div.tip, div.warning {
  margin: 2em ;
  border: medium outset ;
  padding: 1em }

div.admonition p.admonition-title, div.hint p.admonition-title,
div.important p.admonition-title, div.note p.admonition-title,
div.tip p.admonition-title {
  font-weight: bold ;
  font-family: sans-serif }

div.attention p.admonition-title, div.caution p.admonition-title,
div.danger p.admonition-title, div.error p.admonition-title,
div.warning p.admonition-title, .code .error {
  color: red ;
  font-weight: bold ;
  font-family: sans-serif }

/* Uncomment (and remove this text!) to get reduced vertical space in
   compound paragraphs.
div.compound .compound-first, div.compound .compound-middle {
  margin-bottom: 0.5em }

div.compound .compound-last, div.compound .compound-middle {
  margin-top: 0.5em }
*/

div.dedication {
  margin: 2em 5em ;
  text-align: center ;
  font-style: italic }

div.dedication p.topic-title {
  font-weight: bold ;
  font-style: normal }

div.figure {
  margin-left: 2em ;
  margin-right: 2em }

div.footer, div.header {
  clear: both;
  font-size: smaller }

div.line-block {
  display: block ;
  margin-top: 1em ;
  margin-bottom: 1em }

div.line-block div.line-block {
  margin-top: 0 ;
  margin-bottom: 0 ;
  margin-left: 1.5em }

div.sidebar {
  margin: 0 0 0.5em 1em ;
  border: medium outset ;
  padding: 1em ;
  background-color: #ffffee ;
  width: 40% ;
  float: right ;
  clear: right }

div.sidebar p.rubric {
  font-family: sans-serif ;
  font-size: medium }

div.system-messages {
  margin: 5em }

div.system-messages h1 {
  color: red }

div.system-message {
  border: medium outset ;
  padding: 1em }

div.system-message p.system-message-title {
  color: red ;
  font-weight: bold }

div.topic {
  margin: 2em }

h1.section-subtitle, h2.section-subtitle, h3.section-subtitle,
h4.section-subtitle, h5.section-subtitle, h6.section-subtitle {
  margin-top: 0.4em }

h1.title {
  text-align: center }

h2.subtitle {
  text-align: center }

hr.docutils {
  width: 75% }

img.align-left, .figure.align-left, object.align-left, table.align-left {
  clear: left ;
  float: left ;
  margin-right: 1em }

img.align-right, .figure.align-right, object.align-right, table.align-right {
  clear: right ;
  float: right ;
  margin-left: 1em }

img.align-center, .figure.align-center, object.align-center {
  display: block;
  margin-left: auto;
  margin-right: auto;
}

table.align-center {
  margin-left: auto;
  margin-right: auto;
}

.align-left {
  text-align: left }

.align-center {
  clear: both ;
  text-align: center }

.align-right {
  text-align: right }

/* reset inner alignment in figures */
div.align-right {
  text-align: inherit }

/* div.align-center * { */
/*   text-align: left } */

.align-top    {
  vertical-align: top }

.align-middle {
  vertical-align: middle }

.align-bottom {
  vertical-align: bottom }

ol.simple, ul.simple {
  margin-bottom: 1em }

ol.arabic {
  list-style: decimal }

ol.loweralpha {
  list-style: lower-alpha }

ol.upperalpha {
  list-style: upper-alpha }

ol.lowerroman {
  list-style: lower-roman }

ol.upperroman {
  list-style: upper-roman }

p.attribution {
  text-align: right ;
  margin-left: 50% }

p.caption {
  font-style: italic }

p.credits {
  font-style: italic ;
  font-size: smaller }

p.label {
  white-space: nowrap }

p.rubric {
  font-weight: bold ;
  font-size: larger ;
  color: maroon ;
  text-align: center }

p.sidebar-title {
  font-family: sans-serif ;
  font-weight: bold ;
  font-size: larger }

p.sidebar-subtitle {
  font-family: sans-serif ;
  font-weight: bold }

p.topic-title {
  font-weight: bold }

pre.address {
  margin-bottom: 0 ;
  margin-top: 0 ;
  font: inherit }

pre.literal-block, pre.doctest-block, pre.math, pre.code {
  margin-left: 2em ;
  margin-right: 2em }

pre.code .ln { color: grey; } /* line numbers */
pre.code, code { background-color: #eeeeee }
pre.code .comment, code .comment { color: #5C6576 }
pre.code .keyword, code .keyword { color: #3B0D06; font-weight: bold }
pre.code .literal.string, code .literal.string { color: #0C5404 }
pre.code .name.builtin, code .name.builtin { color: #352B84 }
pre.code .deleted, code .deleted { background-color: #DEB0A1}
pre.code .inserted, code .inserted { background-color: #A3D289}

span.classifier {
  font-family: sans-serif ;
  font-style: oblique }

span.classifier-delimiter {
  font-family: sans-serif ;
  font-weight: bold }

span.interpreted {
  font-family: sans-serif }

span.option {
  white-space: nowrap }

span.pre {
  white-space: pre }

span.problematic {
  color: red }

span.section-subtitle {
  /* font-size relative to parent (h1..h6 element) */
  font-size: 80% }

table.citation {
  border-left: solid 1px gray;
  margin-left: 1px }

table.docinfo {
  margin: 2em 4em }

table.docutils {
  margin-top: 0.5em ;
  margin-bottom: 0.5em }

table.footnote {
  border-left: solid 1px black;
  margin-left: 1px }

table.docutils td, table.docutils th,
table.docinfo td, table.docinfo th {
  padding-left: 0.5em ;
  padding-right: 0.5em ;
  vertical-align: top }

table.docutils th.field-name, table.docinfo th.docinfo-name {
  font-weight: bold ;
  text-align: left ;
  white-space: nowrap ;
  padding-left: 0 }

/* "booktabs" style (no vertical lines) */
table.docutils.booktabs {
  border: 0px;
  border-top: 2px solid;
  border-bottom: 2px solid;
  border-collapse: collapse;
}
table.docutils.booktabs * {
  border: 0px;
}
table.docutils.booktabs th {
  border-bottom: thin solid;
  text-align: left;
}

h1 tt.docutils, h2 tt.docutils, h3 tt.docutils,
h4 tt.docutils, h5 tt.docutils, h6 tt.docutils {
  font-size: 100% }

ul.auto-toc {
  list-style-type: none }

</style>
<style type="text/css">

/*
*   math2html: convert LaTeX equations to HTML output.
*
*   Copyright (C) 2009,2010 Alex Fernández
*                 2021      Günter Milde
*
*   Released under the terms of the `2-Clause BSD license'_, in short:
*   Copying and distribution of this file, with or without modification,
*   are permitted in any medium without royalty provided the copyright
*   notice and this notice are preserved.
*   This file is offered as-is, without any warranty.
*
* .. _2-Clause BSD license: http://www.spdx.org/licenses/BSD-2-Clause
*
*   Based on eLyXer: convert LyX source files to HTML output.
*   http://elyxer.nongnu.org/
*
*
* CSS file for LaTeX formulas.
*
* References: http://www.zipcon.net/~swhite/docs/math/math.html
*             http://www.cs.tut.fi/~jkorpela/math/
*/

/* Formulas */
.formula {
	text-align: center;
	margin: 1.2em 0;
	line-height: 1.4;
}
span.formula {
	white-space: nowrap;
}
div.formula {
	padding: 0.5ex;
	margin-left: auto;
	margin-right: auto;
}

/* Basic features */
a.eqnumber {
	display: inline-block;
	float: right;
	clear: right;
	font-weight: bold;
}
span.unknown {
	color: #800000;
}
span.ignored, span.arraydef {
	display: none;
}
.phantom {
	visibility: hidden;
}
.formula i {
	letter-spacing: 0.1ex;
}

/* Alignment */
.align-left, .align-l {
	text-align: left;
}
.align-right, .align-r {
	text-align: right;
}
.align-center, .align-c {
	text-align: center;
}

/* Structures */
span.hspace {
	display: inline-block;
}
span.overline, span.bar {
	text-decoration: overline;
}
.fraction, .fullfraction, .textfraction {
	display: inline-block;
	vertical-align: middle;
	text-align: center;
}
span.formula .fraction,
.textfraction,
span.smallmatrix {
	font-size: 80%;
	line-height: 1;
}
span.numerator {
	display: block;
	line-height: 1;
}
span.denominator {
	display: block;
	line-height: 1;
	padding: 0ex;
	border-top: thin solid;
}
.formula sub, .formula sup {
	font-size: 80%;
}
sup.numerator, sup.unit {
	vertical-align: 80%;
}
sub.denominator, sub.unit {
	vertical-align: -20%;
}
span.smallsymbol {
	font-size: 75%;
	line-height: 75%;
}
span.boldsymbol {
	font-weight: bold;
}
span.sqrt {
	display: inline-block;
	vertical-align: middle;
	padding: 0.1ex;
}
sup.root {
	position: relative;
	left: 1.4ex;
}
span.radical {
	display: inline-block;
	padding: 0ex;
	/* font-size: 160%; for DejaVu, not required with STIX */
	line-height: 100%;
	vertical-align: top;
	vertical-align: middle;
}

span.root {
	display: inline-block;
	border-top: thin solid;
	padding: 0ex;
	vertical-align: middle;
}
div.formula .bigoperator,
.displaystyle .bigoperator,
.displaystyle .bigoperator {
	line-height: 120%;
	font-size: 140%;
	padding-right: 0.2ex;
}
span.fraction .bigoperator,
span.scriptstyle .bigoperator {
	line-height: inherit;
	font-size: inherit;
	padding-right: 0;
}
span.bigdelimiter {
	display: inline-block;
}
span.bigdelimiter.size1 {
	transform: scale(1, 1.2);
	line-height: 1.2;
}
span.bigdelimiter.size2 {
	transform: scale(1, 1.62);
	line-height: 1.62%;

}
span.bigdelimiter.size3 {
	transform: scale(1, 2.05);
	line-height: 2.05%;
}
span.bigdelimiter.size4 {
	transform: scale(1, 2.47);
	line-height: 2.47%;
}
/* vertically stacked sub and superscript */
span.scripts {
	display: inline-table;
	vertical-align: middle;
	padding-right: 0.2ex;
}
.script {
	display: table-row;
	text-align: left;
	line-height: 150%;
}
span.limits {
	display: inline-table;
	vertical-align: middle;
}
.limit {
	display: table-row;
	line-height: 99%;
}
sup.limit, sub.limit {
	line-height: 100%;
}
span.embellished,
span.embellished > .base {
	display: inline-block;
}
span.embellished > sup,
span.embellished > sub {
	display: inline-block;
	font-size: 100%;
	position: relative;
	bottom: 0.3em;
	width: 0px;
}
span.embellished > sub {
	top: 0.4em;
}

/* Environments */
span.array, span.bracketcases, span.binomial, span.environment {
	display: inline-table;
	text-align: center;
	vertical-align: middle;
}
span.arrayrow, span.binomrow {
	display: table-row;
	padding: 0;
	border: 0;
}
span.arraycell, span.bracket, span.case, span.binomcell, span.environmentcell {
	display: table-cell;
	padding: 0ex 0.2ex;
	line-height: 1; /* 99%; */
	border: 0ex;
}
.environment.align > .arrayrow > .arraycell.align-l {
	padding-right: 2em;
}

/* Inline binomials */
span.binom {
	display: inline-block;
	vertical-align: middle;
	text-align: center;
	font-size: 80%;
}
span.binomstack {
	display: block;
	padding: 0em;
}

/* Over- and underbraces */
span.overbrace {
	border-top: 2pt solid;
}
span.underbrace {
	border-bottom: 2pt solid;
}

/* Stackrel */
span.stackrel {
	display: inline-block;
	text-align: center;
}
span.upstackrel {
	display: block;
	padding: 0em;
	font-size: 80%;
	line-height: 64%;
	position: relative;
	top: 0.15em;

}
span.downstackrel {
	display: block;
	vertical-align: bottom;
	padding: 0em;
}

/* Fonts */
.formula {
	font-family: STIX, "DejaVu Serif", "DejaVu Math TeX Gyre", serif;
}
span.radical,   /* ensure correct size of square-root sign */
span.integral { /* upright integral signs for better alignment of indices */
	font-family: "STIXIntegralsUp", STIX;
	/* font-size: 115%; match apparent size with DejaVu */
}
span.bracket {
  /* some "STIX" and "DejaVu Math TeX Gyre" bracket pieces don't fit */
	font-family: "DejaVu Serif", serif;
}
span.mathsf, span.textsf {
	font-family: sans-serif;
}
span.mathrm, span.textrm {
	font-family: STIX, "DejaVu Serif", "DejaVu Math TeX Gyre", serif;
}
span.mathtt, span.texttt {
	font-family: monospace;
}
span.text, span.textnormal,
span.mathsf, span.mathtt, span.mathrm {
	font-style: normal;
}
span.fraktur {
	font-family: "Lucida Blackletter", eufm10, blackletter;
}
span.blackboard {
	font-family: Blackboard, msbm10, serif;
}
span.scriptfont {
	font-family: "Monotype Corsiva", "Apple Chancery", "URW Chancery L", cursive;
	font-style: italic;
}
span.mathscr {
  font-family: MathJax_Script, rsfs10,  cursive;
  font-style: italic;
}
span.textsc {
	font-variant: small-caps;
}
span.textsl {
	font-style: oblique;
}

/* Colors */
span.colorbox {
	display: inline-block;
	padding: 5px;
}
span.fbox {
	display: inline-block;
	border: thin solid black;
	padding: 2px;
}
span.boxed, span.framebox {
	display: inline-block;
	border: thin solid black;
	padding: 5px;
}

</style>
</head>
<body>
<div class="document" id="fourier-neural-operators">
<h1 class="title">Fourier Neural Operators</h1>

<p>This page (which takes about 10 minutes to read), introduces the Fourier neural operator that solves a family of PDEs from scratch.
It the first work that can learn resolution-invariant solution operators on Navier-Stokes equation,
achieving state-of-the-art accuracy among all existing deep learning methods and
up to 1000x faster than traditional solvers.
Also check out the paper <a class="reference external" href="https://arxiv.org/abs/2010.08895">https://arxiv.org/abs/2010.08895</a> and article
<a class="reference external" href="https://www.technologyreview.com/2020/10/30/1011435/ai-fourier-neural-network-cracks-navier-stokes-and-partial-differential-equations/">https://www.technologyreview.com/2020/10/30/1011435/ai-fourier-neural-network-cracks-navier-stokes-and-partial-differential-equations/</a></p>
<div class="section" id="operator-learning">
<h1>Operator learning</h1>
<p>Thinking in continuum gives us an advantage when dealing with PDE.
We want to design mesh-indepedent, resolution-invariant operators.</p>
<p>Problems in science and engineering involve solving
partial differential equations (PDE) systems.
Unfortunately, these PDEs can be very hard.
Traditional PDE solver such as finite element methods (FEM) and finite difference methods (FDM)
rely on discretizing the space into a very fine mesh.
And it can be slow and inefficient.</p>
<p>In the previous doc,
we introduced the neural operators that use neural networks
to learn the solution operators for PDEs.
That is, given the initial conditions or the boundary conditions,
the neural network directly output the solution,
kind of like an image-to-image mapping.</p>
<p>The neural operator is mesh-independent,
different from the standard deep learning methods such as CNN.
It can be trained on one mesh and evaluated on another.
By parameterizing the model in function space,
it learns the continuous function instead of discretized vectors.</p>
<blockquote>
<table border="1" class="docutils">
<colgroup>
<col width="53%" />
<col width="48%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Conventional PDE solvers</th>
<th class="head">Neural operators</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>Solve one instance</td>
<td>Learn a family of PDE</td>
</tr>
<tr><td>Require the explicit form</td>
<td>Black-box, data-driven</td>
</tr>
<tr><td>Speed-accuracy trade-off on resolution</td>
<td>Resolution-invariant, mesh-invariant</td>
</tr>
<tr><td>Slow on fine grids; fast on coarse grids</td>
<td>Slow to train; fast to evaluate</td>
</tr>
</tbody>
</table>
</blockquote>
<p>Operator learning can be taken as an image-to-image problem.
The Fourier layer can be viewed as a substitute for the convolution layer.</p>
</div>
<div class="section" id="framework-of-neural-operators">
<h1>Framework of Neural Operators</h1>
<p>Just like neural networks consist of linear transformations and non-linear activation functions,
neural operators consist of linear operators and non-linear activation operators.</p>
<p>Let <span class="formula"><i>v</i></span> be the input vector, <span class="formula"><i>u</i></span> be the output vector.
A standard deep neural network can be written in the form:</p>
<div class="formula">
<i>u</i> = <span class="stretchy">(</span><i>K</i><sub><i>l</i></sub>∘<i>σ</i><sub><i>l</i></sub>∘⋯∘<i>σ</i><sub>1</sub>∘<i>K</i><sub>0</sub><span class="stretchy">)</span><i>v</i>
</div>
<p>where <span class="formula"><i>K</i></span> are the linear layer or convolution layer,
and <span class="formula"><i>σ</i></span> are the activation function such as ReLU.</p>
<p>The neural operator shares a similar framework.
It’s just now <span class="formula"><i>v</i></span> and <span class="formula"><i>u</i></span> are functions with different discretizations
(say, some inputs are <span class="formula">28×28</span>, some are <span class="formula">256×256</span>,
and some are in triangular mesh).
To deal with functions input, the linear transformation <span class="formula"><i>K</i></span> is formulated as an integral operator.
Let <span class="formula"><i>x</i>, <i>y</i></span> be the points in the domain.</p>
<p>The map <span class="formula"><i>K</i> : <i>v</i><sub><i>t</i></sub> ↦ <i>v</i><sub><i>t</i> + 1</sub></span> is parameterized as</p>
<div class="formula">
<i>v</i>’(<i>x</i>) = <span class="limits"><span class="limit"><span class="bigoperator integral">∫</span></span></span><i>κ</i>(<i>x</i>, <i>y</i>)<i>v</i>(<i>y</i>)<i>dy</i> + <i>W</i><i>v</i>(<i>x</i>)
</div>
<p>Where <span class="formula"><i>κ</i></span> is a kernel function and <span class="formula"><i>W</i></span> is the bias term.</p>
<p>For the Fourier neural operator, we formulate <span class="formula"><i>K</i></span> as a convolution
and implement it by Fourier transformation.</p>
</div>
<div class="section" id="fourier-layer">
<h1>Fourier Layer</h1>
<p>The real-world images have lots of edges and shapes,
so CNN can capture them well with local convolution kernel.
On the other hand, the inputs and outputs of PDEs are continuous functions.
It is more efficient to represent them in Fourier space and do global convolution.</p>
<p>There are two main motivations to use Fourier transformation.
First, it’s fast. A full standard integration of <span class="formula"><i>n</i></span> points has complexity <span class="formula"><i>O</i>(<i>n</i><sup>2</sup>)</span>,
while convolution via Fourier transform is quasilinear.
Second, it’s efficient. The inputs and outputs of PDEs are continuous functions.
So it’s usually more efficient to represent them in Fourier space.</p>
<p>The convolution in the spatial domain is equivalent to the pointwise multiplication in the Fourier domain. To implement the (global) convolution operator,
we first do a Fourier transform, then a linear transform, and an inverse Fourier transform,
As shown in the top part of the figure:</p>
<img alt="/_static/images/fourier_layer.jpg" src="/_static/images/fourier_layer.jpg" style="width: 800px;" />
<dl class="docutils">
<dt>The Fourier layer just consists of three steps:</dt>
<dd><ul class="first last simple">
<li>Fourier transform <span class="formula">ℱ</span></li>
<li>Linear transform on the lower Fourier modes <span class="formula"><i>R</i></span></li>
<li>Inverse Fourier transform <span class="formula">ℱ<sup> − 1</sup></span></li>
</ul>
</dd>
</dl>
<p>We then add the output of the Fourier layer
with the bias term <span class="formula"><i>W</i><i>v</i></span> (a linear transformation)
and apply the activation function <span class="formula"><i>σ</i></span>.
Simple as it is.</p>
<p>In practice, it’s usually sufficient to only take the lower frequency modes
and truncate out these higher frequency modes.
Therefore, we apply the linear transformation on the lower frequency modes
and set the higher modes to zeros.</p>
<p>Notice the activation functions shall be applied on the spatial domain.
They help to recover the Higher frequency modes and non-periodic boundary
which are left out in the Fourier layers.
Therefore it’s necessary to the Fourier transform and its inverse at each layer.</p>
</div>
<div class="section" id="implementation">
<h1>Implementation</h1>
<p>Here is a simple example of the 2d Fourier layer
based on PyTorch's fast Fourier transform <code>torch.fft.rfft()</code> and <code>torch.fft.irfft()</code>:</p>
<div class="system-message">
<p class="system-message-title">System Message: ERROR/3 (<tt class="docutils">/Users/lizongyi/Documents/website/neuraloperator/doc/source/user_guide/fno.rst</tt>, line 133)</p>
<p>Error in &quot;code&quot; directive:
maximum 1 argument(s) allowed, 20 supplied.</p>
<pre class="literal-block">
.. code:: python
    def forward(self, x):
        batchsize = x.shape[0]
        #Compute Fourier coeffcients up to factor of e^(- something constant)
        x_ft = torch.fft.rfft2(x)

        # Multiply relevant Fourier modes
        out_ft = torch.zeros(batchsize, self.out_channels,  x.size(-2), x.size(-1)//2 + 1, dtype=torch.cfloat, device=x.device)
        out_ft[:, :, :self.modes1, :self.modes2] = \
            self.compl_mul2d(x_ft[:, :, :self.modes1, :self.modes2], self.weights1)
        out_ft[:, :, -self.modes1:, :self.modes2] = \
            self.compl_mul2d(x_ft[:, :, -self.modes1:, :self.modes2], self.weights2)

        #Return to physical space
        x = torch.fft.irfft2(out_ft, s=(x.size(-2), x.size(-1)))
        return x

</pre>
</div>
<p>where the input <code>v_ has the shape (N,C,H,W),
:code:`self.weights1</code> and <code>self.weights2</code> are the weight matrices;
<code>self.mode1</code> and _self.mode2` truncate the lower frequency modes;
and :code:`compl_mul2d()1 is the matrix multiplication for complex numbers.</p>
<img alt="/_static/images/filters.jpg" src="/_static/images/filters.jpg" style="width: 800px;" />
<p>Filters in convolution neural networks are usually local.
They are good to capture local patterns such as edges and shapes.
Fourier filters are global sinusoidal functions.
They are better for representing continuous functions.</p>
<p><strong>Higher frequency modes and non-periodic boundary</strong>
The Fourier layer on its own loses higher frequency modes
and works only with periodic boundary conditions.
However, the Fourier neural operator as a whole does not have these limitations
(examples shown in the experiments).
The encoder-decoder structure
helps to recover the higher Fourier modes.
And the bias term <span class="formula"><i>W</i></span>
helps to recover the non-periodic boundary.</p>
<p><strong>Complexity</strong>
The Fourier layer has a quasilinear complexity.
Denote the number of points (pixels) <span class="formula"><i>n</i></span> and truncating at <span class="formula"><i>k</i><sub><i>max</i></sub></span> frequency modes.
The multiplication has complexity <span class="formula"><i>O</i>(<i>k</i><sub><i>max</i></sub>) &lt; <i>O</i>(<i>n</i>)</span> .
The majority of the computational cost lies in computing the Fourier transform and its inverse.
General Fourier transforms have complexity <span class="formula"><i>O</i>(<i>n</i><sup>2</sup>)</span>,
however, since we truncate the series the complexity is in fact <span class="formula"><i>O</i>(<i>n</i><i>k</i><sub><i>max</i></sub>)</span>,
while the FFT has complexity <span class="formula"><i>O</i>(<i>n</i>log<i>n</i>)</span>.</p>
<p><strong>resolution-invariance&quot;&gt;Resolution-invariance</strong>
The Fourier layers are discretization-invariant,
because they can learn from and evaluate functions
which are discretized in an arbitrary way.
Since parameters are learned directly in Fourier space,
resolving the functions in physical space simply amounts to projecting on the basis
of wave functions which are well-defined everywhere on the space.
This allows us to transfer among discretization.
If implemented with standard FFT, then it will be restricted to uniform mesh,
but still resolution-invariant.</p>
</div>
<div class="section" id="experiments">
<h1>Experiments</h1>
<p><strong>Burgers Equation</strong>
The 1-d Burgers’ equation is a non-linear PDE with various applications
including modeling the one-dimensional flow of a viscous fluid. It takes the form</p>
<div class="formula">
∂<sub><i>t</i></sub><i>u</i>(<i>x</i>, <i>t</i>) + ∂<sub><i>x</i></sub>(<i>u</i><sup>2</sup>(<i>x</i>, <i>t</i>) ⁄ 2) = <i>ν</i>∂<sub><i>xx</i></sub><i>u</i>(<i>x</i>, <i>t</i>),   <i>x</i> ∈ (0, 1), <i>t</i> ∈ (0, 1]
</div>
<div class="formula">
<i>u</i>(<i>x</i>, 0) = <i>u</i><sub>0</sub>(<i>x</i>),       <i>x</i> ∈ (0, 1)
</div>
<p>with periodic boundary conditions where <span class="formula"><i>u</i><sub>0</sub> ∈ <i>L</i><span class="scripts"><sup class="script">2</sup><sub class="script"><span class="text"> per</span></sub></span>((0, 1);ℝ)</span>
is the initial condition and <span class="formula"><i>ν</i> ∈ ℝ<sub> + </sub></span> is the viscosity coefficient.
We aim to learn the operator mapping the initial condition to the solution
at time one, defined by <span class="formula"><i>u</i><sub>0</sub> ↦ <i>u</i>(⋅, 1)</span> for any <span class="formula"><i>r</i> &gt; 0</span>.</p>
<blockquote>
<table border="1" class="docutils">
<colgroup>
<col width="17%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Networks</th>
<th class="head">s=256</th>
<th class="head">s=512</th>
<th class="head">s=1024</th>
<th class="head">s=2048</th>
<th class="head">s=4096</th>
<th class="head">s=8192</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>FCN</td>
<td>0.0958</td>
<td>0.1407</td>
<td>0.1877</td>
<td>0.2313</td>
<td>0.2855</td>
<td>0.3238</td>
</tr>
<tr><td>PCA+NN</td>
<td>0.0398</td>
<td>0.0395</td>
<td>0.0391</td>
<td>0.0383</td>
<td>0.0392</td>
<td>0.0393</td>
</tr>
<tr><td>LNO</td>
<td>0.0212</td>
<td>0.0221</td>
<td>0.0217</td>
<td>0.0219</td>
<td>0.0200</td>
<td>0.0189</td>
</tr>
<tr><td>FNO</td>
<td>0.0149</td>
<td>0.0158</td>
<td>0.0160</td>
<td>0.0146</td>
<td>0.0142</td>
<td>0.0139</td>
</tr>
</tbody>
</table>
</blockquote>
<p><strong>Darcy Flow</strong></p>
<p>We consider the steady-state of the 2-d Darcy Flow equation
on the unit box which is the second order, linear, elliptic PDE</p>
<div class="formula">
 − ∇⋅(<i>a</i>(<i>x</i>)∇<i>u</i>(<i>x</i>)) = <i>f</i>(<i>x</i>)  <i>x</i> ∈ (0, 1)<sup>2</sup>
</div>
<div class="formula">
<i>u</i>(<i>x</i>) = 0     <i>x</i> ∈ ∂(0, 1)<sup>2</sup>
</div>
<p>with a Dirichlet boundary where <span class="formula"><i>a</i> ∈ <i>L</i><sup>∞</sup>((0, 1)<sup>2</sup>;ℝ<sub> + </sub>)</span>  is the diffusion coefficient and <span class="formula"><i>f</i> ∈ <i>L</i><sup>2</sup>((0, 1)<sup>2</sup>;ℝ)</span> is the forcing function.
This PDE has numerous applications including modeling the pressure of the subsurface flow,
the deformation of linearly elastic materials, and the electric potential in conductive materials.
We are interested in learning the operator mapping the diffusion coefficient to the solution,
defined by <span class="formula"><i>a</i> ↦ <i>u</i></span>. Note that although the PDE is linear, the solution operator is not.</p>
<blockquote>
<table border="1" class="docutils">
<colgroup>
<col width="24%" />
<col width="19%" />
<col width="19%" />
<col width="19%" />
<col width="19%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Networks</th>
<th class="head">s=85</th>
<th class="head">s=141</th>
<th class="head">s=211</th>
<th class="head">s=421</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>FCN</td>
<td>0.0253</td>
<td>0.0493</td>
<td>0.0727</td>
<td>0.1097</td>
</tr>
<tr><td>PCA+NN</td>
<td>0.0299</td>
<td>0.0298</td>
<td>0.0298</td>
<td>0.0299</td>
</tr>
<tr><td>RBM</td>
<td>0.0244</td>
<td>0.0251</td>
<td>0.0255</td>
<td>0.0259</td>
</tr>
<tr><td>LNO</td>
<td>0.0520</td>
<td>0.0461</td>
<td>0.0445</td>
<td><ul class="first last simple">
<li></li>
</ul>
</td>
</tr>
<tr><td>FNO</td>
<td>0.0108</td>
<td>0.0109</td>
<td>0.0109</td>
<td>0.0098</td>
</tr>
</tbody>
</table>
</blockquote>
<img alt="/_static/images/fourier_error.jpg" src="/_static/images/fourier_error.jpg" style="width: 800px;" />
<p>Benchmarks for time-independent problems (Burgers and Darcy):</p>
<blockquote>
<ul class="simple">
<li>NN: a simple point-wise feedforward neural network.</li>
<li>RBM: the classical Reduced Basis Method (using a POD basis).</li>
<li>FCN: a the-state-of-the-art neural network architecture based on Fully Convolution Networks.</li>
<li>PCANN: an operator method using PCA as an autoencoder on both the input and output data and interpolating the latent spaces with a neural network.</li>
<li>GNO: the original graph neural operator.</li>
<li>MGNO: the multipole graph neural operator.</li>
<li>LNO: a neural operator method based on the low-rank decomposition of the kernel.</li>
<li>FNO: the newly purposed Fourier neural operator.</li>
</ul>
</blockquote>
<p><strong>Navier-Stokes Equation</strong></p>
<p>We consider the 2-d Navier-Stokes equation for a viscous,
incompressible fluid in vorticity form on the unit torus:</p>
<div class="formula">
∂<sub><i>t</i></sub><i>w</i>(<i>x</i>, <i>t</i>) + <i>u</i>(<i>x</i>, <i>t</i>)⋅∇<i>w</i>(<i>x</i>, <i>t</i>) = <i>ν</i>Δ<i>w</i>(<i>x</i>, <i>t</i>) + <i>f</i>(<i>x</i>),   <i>x</i> ∈ (0, 1)<sup>2</sup>, <i>t</i> ∈ (0, <i>T</i>]
</div>
<div class="formula">
∇⋅<i>u</i>(<i>x</i>, <i>t</i>) = 0,     <i>x</i> ∈ (0, 1)<sup>2</sup>, <i>t</i> ∈ [0, <i>T</i>]
</div>
<div class="formula">
<i>w</i>(<i>x</i>, 0) = <i>w</i><sub>0</sub>(<i>x</i>),       <i>x</i> ∈ (0, 1)<sup>2</sup>
</div>
<p>where <span class="formula"><i>u</i></span> is the velocity field,
<span class="formula"><i>w</i> = ∇×<i>u</i></span> is the vorticity,
<span class="formula"><i>w</i><sub>0</sub></span> is the initial vorticity,&lt;br /&gt;
<span class="formula"><i>ν</i></span> is the viscosity coefficient,
and <span class="formula"><i>f</i></span> is the forcing function.
We are interested in learning the operator mapping the vorticity up to time 10
to the vorticity up to some later time <span class="formula"><i>T</i> &gt; 10</span>,
defined by <span class="formula"><i>w</i>|<sub>(0, 1)<sup>2</sup>×[0, 10]</sub> ↦ <i>w</i>|<sub>(0, 1)<sup>2</sup>×(10, <i>T</i>]</sub></span>.
We experiment with the viscosities
<span class="formula"><i>ν</i> = 1<span class="mathrm"> e</span> − 3, 1<span class="mathrm"> e</span> − 4, 1<span class="mathrm"> e</span> − 5</span>,
decreasing the final time <span class="formula"><i>T</i></span> as the dynamic becomes chaotic.</p>
<blockquote>
<table border="1" class="docutils">
<colgroup>
<col width="14%" />
<col width="19%" />
<col width="25%" />
<col width="14%" />
<col width="14%" />
<col width="14%" />
</colgroup>
<thead valign="bottom">
<tr><th class="head">Configs</th>
<th class="head">Parameters</th>
<th class="head">Time per epoch</th>
<th class="head">nu=1e-3</th>
<th class="head">nu=1e-4</th>
<th class="head">nu=1e-5</th>
</tr>
</thead>
<tbody valign="top">
<tr><td>FNO-3D</td>
<td>6,558,537</td>
<td>38.99s</td>
<td>0.0086</td>
<td>0.0820</td>
<td>0.1893</td>
</tr>
<tr><td>FNO-2D</td>
<td>414,517</td>
<td>127.80s</td>
<td>0.0128</td>
<td>0.0973</td>
<td>0.1556</td>
</tr>
<tr><td>U-Net</td>
<td>24,950,491</td>
<td>48.67s</td>
<td>0.0245</td>
<td>0.1190</td>
<td>0.1982</td>
</tr>
<tr><td>TF-Net</td>
<td>7,451,724</td>
<td>47.21s</td>
<td>0.0225</td>
<td>0.1168</td>
<td>0.2268</td>
</tr>
<tr><td>ResNet</td>
<td>266,641</td>
<td>78.47s</td>
<td>0.0701</td>
<td>0.2311</td>
<td>0.2753</td>
</tr>
</tbody>
</table>
</blockquote>
<img alt="/_static/images/fourier_ns1e4.jpg" src="/_static/images/fourier_ns1e4.jpg" style="width: 800px;" />
<p>Benchmarks for time-dependent problems (Navier-Stokes):</p>
<blockquote>
<ul class="simple">
<li>ResNet: 18 layers of 2-d convolution with residual connections.</li>
<li>U-Net: A popular choice for image-to-image regression tasks consisting of four blocks with 2-d convolutions and deconvolutions.</li>
<li>TF-Net: A network designed for learning turbulent flows based on a combination of spatial and temporal convolutions.</li>
<li>FNO-2d: 2-d Fourier neural operator with an RNN structure in time.</li>
<li>FNO-3d: 3-d Fourier neural operator that directly convolves in space-time.</li>
</ul>
</blockquote>
<p>The FNO-3D has the best performance
when there is sufficient data
(<span class="formula"><i>ν</i> = 1<span class="mathrm"> e</span> − 3, <i>N</i> = 1000</span> and <span class="formula"><i>ν</i> = 1<span class="mathrm"> e</span> − 4, <i>N</i> = 10000</span>).
For the configurations where the amount of data is insufficient
(<span class="formula"><i>ν</i> = 1<span class="mathrm"> e</span> − 4, <i>N</i> = 1000</span> and <span class="formula"><i>ν</i> = 1<span class="mathrm"> e</span> − 5, <i>N</i> = 1000</span>),
all methods have <span class="formula"> &gt; 15%</span> error with FNO-2D achieving the lowest.
Note that we only present results for spatial resolution <span class="formula">64×64</span>
since all benchmarks we compare against are designed for this resolution.
Increasing it degrades their performance while FNO achieves the same errors.</p>
<p>FNO-2D, U-Net, TF-Net, and ResNet all use 2D-convolution in the spatial domain
and recurrently propagate in the time domain (2D+RNN).
On the other hand, FNO-3D performs convolution in space-time.</p>
<p><strong>Bayesian Inverse Problem</strong></p>
<p>In this experiment, we use a function space Markov chain Monte Carlo (MCMC) method
to draw samples from the posterior distribution of the initial vorticity
in Navier-Stokes given sparse, noisy observations at time <span class="formula"><i>T</i> = 50</span>.
We compare the Fourier neural operator acting as a surrogate model
with the traditional solvers used to generate our train-test data (both run on GPU).
We generate 25,000 samples from the posterior (with a 5,000 sample burn-in period),
requiring 30,000 evaluations of the forward operator.</p>
<img alt="/_static/images/fourier_bayesian.jpg" src="/_static/images/fourier_bayesian.jpg" style="width: 800px;" />
<p>The top left panel shows the true initial vorticity while the bottom left panel shows
the true observed vorticity at <span class="formula"><i>T</i> = 50</span> with black dots indicating
the locations of the observation points placed on a <span class="formula">7×7</span> grid.
The top middle panel shows the posterior mean of the initial vorticity
given the noisy observations estimated with MCMC using the traditional solver,
while the top right panel shows the same thing but using FNO as a surrogate model.
The bottom middle and right panels show the vorticity at <span class="formula"><i>T</i> = 50</span>
when the respective approximate posterior means are used as initial conditions.</p>
</div>
<div class="section" id="conclusion">
<h1>Conclusion</h1>
<p>We propose a neural operator based on Fourier Transformation.
It is the first work that learns the resolution-invariant solution operator
for the family of Navier-Stokes equation in the turbulent regime,
where previous graph-based neural operators do not converge.
By construction, the method shares the same learned network parameters
irrespective of the dis- cretization used on the input and output spaces.
It can do zero-shot super-resolution: trained on a lower resolution
directly evaluated on a higher resolution.
The proposed method consistently outperforms all existing deep learning methods for parametric PDEs.
It achieves error rates that are <span class="formula">30%</span> lower on Burgers’ Equation,
<span class="formula">60%</span> lower on Darcy Flow, and <span class="formula">30%</span> lower on Navier Stokes
(turbulent regime with Reynolds number <span class="formula">10000</span>).
On a <span class="formula">256×256</span> grid,
the Fourier neural operator has an inference time of only <span class="formula">0.005</span>
compared to the <span class="formula">2.2<i>s</i></span> of the pseudo-spectral method used to solve Navier-Stokes.</p>
</div>
<div class="section" id="references">
<h1>References</h1>
<table class="docutils footnote" frame="void" id="footnote-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label">[1]</td><td>Fourier Neural Operator for Parametric Partial Differential Equations,
Zongyi Li and Nikola Kovachki and Kamyar Azizzadenesheli
and Burigede Liu and Kaushik Bhattacharya and Andrew Stuart and Anima Anandkumar, 2020.</td></tr>
</tbody>
</table>
</div>
</div>
</body>
</html>
