<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="Docutils 0.19: https://docutils.sourceforge.io/" />
<title>Neural Operators: an Introduction</title>
<style type="text/css">

/*
:Author: David Goodger (goodger@python.org)
:Id: $Id: html4css1.css 8954 2022-01-20 10:10:25Z milde $
:Copyright: This stylesheet has been placed in the public domain.

Default cascading style sheet for the HTML output of Docutils.

See https://docutils.sourceforge.io/docs/howto/html-stylesheets.html for how to
customize this style sheet.
*/

/* used to remove borders from tables and images */
.borderless, table.borderless td, table.borderless th {
  border: 0 }

table.borderless td, table.borderless th {
  /* Override padding for "table.docutils td" with "! important".
     The right padding separates the table cells. */
  padding: 0 0.5em 0 0 ! important }

.first {
  /* Override more specific margin styles with "! important". */
  margin-top: 0 ! important }

.last, .with-subtitle {
  margin-bottom: 0 ! important }

.hidden {
  display: none }

.subscript {
  vertical-align: sub;
  font-size: smaller }

.superscript {
  vertical-align: super;
  font-size: smaller }

a.toc-backref {
  text-decoration: none ;
  color: black }

blockquote.epigraph {
  margin: 2em 5em ; }

dl.docutils dd {
  margin-bottom: 0.5em }

object[type="image/svg+xml"], object[type="application/x-shockwave-flash"] {
  overflow: hidden;
}

/* Uncomment (and remove this text!) to get bold-faced definition list terms
dl.docutils dt {
  font-weight: bold }
*/

div.abstract {
  margin: 2em 5em }

div.abstract p.topic-title {
  font-weight: bold ;
  text-align: center }

div.admonition, div.attention, div.caution, div.danger, div.error,
div.hint, div.important, div.note, div.tip, div.warning {
  margin: 2em ;
  border: medium outset ;
  padding: 1em }

div.admonition p.admonition-title, div.hint p.admonition-title,
div.important p.admonition-title, div.note p.admonition-title,
div.tip p.admonition-title {
  font-weight: bold ;
  font-family: sans-serif }

div.attention p.admonition-title, div.caution p.admonition-title,
div.danger p.admonition-title, div.error p.admonition-title,
div.warning p.admonition-title, .code .error {
  color: red ;
  font-weight: bold ;
  font-family: sans-serif }

/* Uncomment (and remove this text!) to get reduced vertical space in
   compound paragraphs.
div.compound .compound-first, div.compound .compound-middle {
  margin-bottom: 0.5em }

div.compound .compound-last, div.compound .compound-middle {
  margin-top: 0.5em }
*/

div.dedication {
  margin: 2em 5em ;
  text-align: center ;
  font-style: italic }

div.dedication p.topic-title {
  font-weight: bold ;
  font-style: normal }

div.figure {
  margin-left: 2em ;
  margin-right: 2em }

div.footer, div.header {
  clear: both;
  font-size: smaller }

div.line-block {
  display: block ;
  margin-top: 1em ;
  margin-bottom: 1em }

div.line-block div.line-block {
  margin-top: 0 ;
  margin-bottom: 0 ;
  margin-left: 1.5em }

div.sidebar {
  margin: 0 0 0.5em 1em ;
  border: medium outset ;
  padding: 1em ;
  background-color: #ffffee ;
  width: 40% ;
  float: right ;
  clear: right }

div.sidebar p.rubric {
  font-family: sans-serif ;
  font-size: medium }

div.system-messages {
  margin: 5em }

div.system-messages h1 {
  color: red }

div.system-message {
  border: medium outset ;
  padding: 1em }

div.system-message p.system-message-title {
  color: red ;
  font-weight: bold }

div.topic {
  margin: 2em }

h1.section-subtitle, h2.section-subtitle, h3.section-subtitle,
h4.section-subtitle, h5.section-subtitle, h6.section-subtitle {
  margin-top: 0.4em }

h1.title {
  text-align: center }

h2.subtitle {
  text-align: center }

hr.docutils {
  width: 75% }

img.align-left, .figure.align-left, object.align-left, table.align-left {
  clear: left ;
  float: left ;
  margin-right: 1em }

img.align-right, .figure.align-right, object.align-right, table.align-right {
  clear: right ;
  float: right ;
  margin-left: 1em }

img.align-center, .figure.align-center, object.align-center {
  display: block;
  margin-left: auto;
  margin-right: auto;
}

table.align-center {
  margin-left: auto;
  margin-right: auto;
}

.align-left {
  text-align: left }

.align-center {
  clear: both ;
  text-align: center }

.align-right {
  text-align: right }

/* reset inner alignment in figures */
div.align-right {
  text-align: inherit }

/* div.align-center * { */
/*   text-align: left } */

.align-top    {
  vertical-align: top }

.align-middle {
  vertical-align: middle }

.align-bottom {
  vertical-align: bottom }

ol.simple, ul.simple {
  margin-bottom: 1em }

ol.arabic {
  list-style: decimal }

ol.loweralpha {
  list-style: lower-alpha }

ol.upperalpha {
  list-style: upper-alpha }

ol.lowerroman {
  list-style: lower-roman }

ol.upperroman {
  list-style: upper-roman }

p.attribution {
  text-align: right ;
  margin-left: 50% }

p.caption {
  font-style: italic }

p.credits {
  font-style: italic ;
  font-size: smaller }

p.label {
  white-space: nowrap }

p.rubric {
  font-weight: bold ;
  font-size: larger ;
  color: maroon ;
  text-align: center }

p.sidebar-title {
  font-family: sans-serif ;
  font-weight: bold ;
  font-size: larger }

p.sidebar-subtitle {
  font-family: sans-serif ;
  font-weight: bold }

p.topic-title {
  font-weight: bold }

pre.address {
  margin-bottom: 0 ;
  margin-top: 0 ;
  font: inherit }

pre.literal-block, pre.doctest-block, pre.math, pre.code {
  margin-left: 2em ;
  margin-right: 2em }

pre.code .ln { color: grey; } /* line numbers */
pre.code, code { background-color: #eeeeee }
pre.code .comment, code .comment { color: #5C6576 }
pre.code .keyword, code .keyword { color: #3B0D06; font-weight: bold }
pre.code .literal.string, code .literal.string { color: #0C5404 }
pre.code .name.builtin, code .name.builtin { color: #352B84 }
pre.code .deleted, code .deleted { background-color: #DEB0A1}
pre.code .inserted, code .inserted { background-color: #A3D289}

span.classifier {
  font-family: sans-serif ;
  font-style: oblique }

span.classifier-delimiter {
  font-family: sans-serif ;
  font-weight: bold }

span.interpreted {
  font-family: sans-serif }

span.option {
  white-space: nowrap }

span.pre {
  white-space: pre }

span.problematic {
  color: red }

span.section-subtitle {
  /* font-size relative to parent (h1..h6 element) */
  font-size: 80% }

table.citation {
  border-left: solid 1px gray;
  margin-left: 1px }

table.docinfo {
  margin: 2em 4em }

table.docutils {
  margin-top: 0.5em ;
  margin-bottom: 0.5em }

table.footnote {
  border-left: solid 1px black;
  margin-left: 1px }

table.docutils td, table.docutils th,
table.docinfo td, table.docinfo th {
  padding-left: 0.5em ;
  padding-right: 0.5em ;
  vertical-align: top }

table.docutils th.field-name, table.docinfo th.docinfo-name {
  font-weight: bold ;
  text-align: left ;
  white-space: nowrap ;
  padding-left: 0 }

/* "booktabs" style (no vertical lines) */
table.docutils.booktabs {
  border: 0px;
  border-top: 2px solid;
  border-bottom: 2px solid;
  border-collapse: collapse;
}
table.docutils.booktabs * {
  border: 0px;
}
table.docutils.booktabs th {
  border-bottom: thin solid;
  text-align: left;
}

h1 tt.docutils, h2 tt.docutils, h3 tt.docutils,
h4 tt.docutils, h5 tt.docutils, h6 tt.docutils {
  font-size: 100% }

ul.auto-toc {
  list-style-type: none }

</style>
<style type="text/css">

/*
*   math2html: convert LaTeX equations to HTML output.
*
*   Copyright (C) 2009,2010 Alex Fernández
*                 2021      Günter Milde
*
*   Released under the terms of the `2-Clause BSD license'_, in short:
*   Copying and distribution of this file, with or without modification,
*   are permitted in any medium without royalty provided the copyright
*   notice and this notice are preserved.
*   This file is offered as-is, without any warranty.
*
* .. _2-Clause BSD license: http://www.spdx.org/licenses/BSD-2-Clause
*
*   Based on eLyXer: convert LyX source files to HTML output.
*   http://elyxer.nongnu.org/
*
*
* CSS file for LaTeX formulas.
*
* References: http://www.zipcon.net/~swhite/docs/math/math.html
*             http://www.cs.tut.fi/~jkorpela/math/
*/

/* Formulas */
.formula {
	text-align: center;
	margin: 1.2em 0;
	line-height: 1.4;
}
span.formula {
	white-space: nowrap;
}
div.formula {
	padding: 0.5ex;
	margin-left: auto;
	margin-right: auto;
}

/* Basic features */
a.eqnumber {
	display: inline-block;
	float: right;
	clear: right;
	font-weight: bold;
}
span.unknown {
	color: #800000;
}
span.ignored, span.arraydef {
	display: none;
}
.phantom {
	visibility: hidden;
}
.formula i {
	letter-spacing: 0.1ex;
}

/* Alignment */
.align-left, .align-l {
	text-align: left;
}
.align-right, .align-r {
	text-align: right;
}
.align-center, .align-c {
	text-align: center;
}

/* Structures */
span.hspace {
	display: inline-block;
}
span.overline, span.bar {
	text-decoration: overline;
}
.fraction, .fullfraction, .textfraction {
	display: inline-block;
	vertical-align: middle;
	text-align: center;
}
span.formula .fraction,
.textfraction,
span.smallmatrix {
	font-size: 80%;
	line-height: 1;
}
span.numerator {
	display: block;
	line-height: 1;
}
span.denominator {
	display: block;
	line-height: 1;
	padding: 0ex;
	border-top: thin solid;
}
.formula sub, .formula sup {
	font-size: 80%;
}
sup.numerator, sup.unit {
	vertical-align: 80%;
}
sub.denominator, sub.unit {
	vertical-align: -20%;
}
span.smallsymbol {
	font-size: 75%;
	line-height: 75%;
}
span.boldsymbol {
	font-weight: bold;
}
span.sqrt {
	display: inline-block;
	vertical-align: middle;
	padding: 0.1ex;
}
sup.root {
	position: relative;
	left: 1.4ex;
}
span.radical {
	display: inline-block;
	padding: 0ex;
	/* font-size: 160%; for DejaVu, not required with STIX */
	line-height: 100%;
	vertical-align: top;
	vertical-align: middle;
}

span.root {
	display: inline-block;
	border-top: thin solid;
	padding: 0ex;
	vertical-align: middle;
}
div.formula .bigoperator,
.displaystyle .bigoperator,
.displaystyle .bigoperator {
	line-height: 120%;
	font-size: 140%;
	padding-right: 0.2ex;
}
span.fraction .bigoperator,
span.scriptstyle .bigoperator {
	line-height: inherit;
	font-size: inherit;
	padding-right: 0;
}
span.bigdelimiter {
	display: inline-block;
}
span.bigdelimiter.size1 {
	transform: scale(1, 1.2);
	line-height: 1.2;
}
span.bigdelimiter.size2 {
	transform: scale(1, 1.62);
	line-height: 1.62%;

}
span.bigdelimiter.size3 {
	transform: scale(1, 2.05);
	line-height: 2.05%;
}
span.bigdelimiter.size4 {
	transform: scale(1, 2.47);
	line-height: 2.47%;
}
/* vertically stacked sub and superscript */
span.scripts {
	display: inline-table;
	vertical-align: middle;
	padding-right: 0.2ex;
}
.script {
	display: table-row;
	text-align: left;
	line-height: 150%;
}
span.limits {
	display: inline-table;
	vertical-align: middle;
}
.limit {
	display: table-row;
	line-height: 99%;
}
sup.limit, sub.limit {
	line-height: 100%;
}
span.embellished,
span.embellished > .base {
	display: inline-block;
}
span.embellished > sup,
span.embellished > sub {
	display: inline-block;
	font-size: 100%;
	position: relative;
	bottom: 0.3em;
	width: 0px;
}
span.embellished > sub {
	top: 0.4em;
}

/* Environments */
span.array, span.bracketcases, span.binomial, span.environment {
	display: inline-table;
	text-align: center;
	vertical-align: middle;
}
span.arrayrow, span.binomrow {
	display: table-row;
	padding: 0;
	border: 0;
}
span.arraycell, span.bracket, span.case, span.binomcell, span.environmentcell {
	display: table-cell;
	padding: 0ex 0.2ex;
	line-height: 1; /* 99%; */
	border: 0ex;
}
.environment.align > .arrayrow > .arraycell.align-l {
	padding-right: 2em;
}

/* Inline binomials */
span.binom {
	display: inline-block;
	vertical-align: middle;
	text-align: center;
	font-size: 80%;
}
span.binomstack {
	display: block;
	padding: 0em;
}

/* Over- and underbraces */
span.overbrace {
	border-top: 2pt solid;
}
span.underbrace {
	border-bottom: 2pt solid;
}

/* Stackrel */
span.stackrel {
	display: inline-block;
	text-align: center;
}
span.upstackrel {
	display: block;
	padding: 0em;
	font-size: 80%;
	line-height: 64%;
	position: relative;
	top: 0.15em;

}
span.downstackrel {
	display: block;
	vertical-align: bottom;
	padding: 0em;
}

/* Fonts */
.formula {
	font-family: STIX, "DejaVu Serif", "DejaVu Math TeX Gyre", serif;
}
span.radical,   /* ensure correct size of square-root sign */
span.integral { /* upright integral signs for better alignment of indices */
	font-family: "STIXIntegralsUp", STIX;
	/* font-size: 115%; match apparent size with DejaVu */
}
span.bracket {
  /* some "STIX" and "DejaVu Math TeX Gyre" bracket pieces don't fit */
	font-family: "DejaVu Serif", serif;
}
span.mathsf, span.textsf {
	font-family: sans-serif;
}
span.mathrm, span.textrm {
	font-family: STIX, "DejaVu Serif", "DejaVu Math TeX Gyre", serif;
}
span.mathtt, span.texttt {
	font-family: monospace;
}
span.text, span.textnormal,
span.mathsf, span.mathtt, span.mathrm {
	font-style: normal;
}
span.fraktur {
	font-family: "Lucida Blackletter", eufm10, blackletter;
}
span.blackboard {
	font-family: Blackboard, msbm10, serif;
}
span.scriptfont {
	font-family: "Monotype Corsiva", "Apple Chancery", "URW Chancery L", cursive;
	font-style: italic;
}
span.mathscr {
  font-family: MathJax_Script, rsfs10,  cursive;
  font-style: italic;
}
span.textsc {
	font-variant: small-caps;
}
span.textsl {
	font-style: oblique;
}

/* Colors */
span.colorbox {
	display: inline-block;
	padding: 5px;
}
span.fbox {
	display: inline-block;
	border: thin solid black;
	padding: 2px;
}
span.boxed, span.framebox {
	display: inline-block;
	border: thin solid black;
	padding: 5px;
}

</style>
</head>
<body>
<div class="document" id="neural-operators-an-introduction">
<h1 class="title">Neural Operators: an Introduction</h1>

<p>Here, we introduce neural operators, a class of models that learn
mappings between function spaces and solve partial differential equations.
You can also check out the papers <a class="footnote-reference" href="#footnote-1" id="footnote-reference-1">[1]</a> and <a class="footnote-reference" href="#footnote-2" id="footnote-reference-2">[2]</a> for more formal derivations,
as well as the blog post <a class="footnote-reference" href="#footnote-3" id="footnote-reference-3">[3]</a>.</p>
<div class="section" id="introduction">
<h1>Introduction</h1>
<p>Scientific computations are expensive.
It could take days and months for numerical solvers to simulate fluid dynamics and many-body motions.
Because to achieve good accuracy,
the numerical solvers need to discretize the space and time into very fine grids
and solve a great number of equations on the grids.
Recently, people are developing data-driven methods based on machine learning techniques such as deep learning.
Instead of directly solving the problems, data-driven solvers learn from the data of the problems and solutions.
When querying new problems, data-driven solvers can directly give predictions based on the data.
Since they don’t need to discretize the space into very small pieces and solve all these equations,
these data-driven solvers are usually much faster compared to traditional numerical solvers.</p>
<p>However, data-driven solvers are subject to the quality of the data given.
If the training data is not good enough, they can’t make good predictions.
In scientific computing, usually, the training data are generated by the traditional numerical solvers.
And to generate good enough data, it still takes days and months for these numerical solvers.
Sometime, data are observed from experiments and there are just no good training data.
Especially, people consider neural networks as interpolators which may not be able to extrapolate.
It is unclear whether neural networks can generalize to unseen data.
So if the training data are of one resolution,
the learned solvers can only solve the problem in this specific resolution.
In general, generalization is a crucial problem in machine learning.
It becomes a trade-off: these machine learning based methods make evaluation easier,
but the training process could be even more painful.</p>
<p>To dealing with this problem, we purpose operator learning. By encoding certain structures,
we let the neural network learn the mapping of functions and generalize among different resolutions.
As a result, we can first use a numerical method generated some less-accurate, low-resolution data,
but the learned solver is still able to give reasonable, high-resolution predictions.
In some sense, both training and evaluation can be pain-free.</p>
</div>
<div class="section" id="operator-learning">
<h1>Operator Learning</h1>
<p>In mathematics, operators are usually referring to the mappings between function spaces.
You most likely have already encountered some operators.
For example, the differentiation and integration are operators.
When we take the derivative  or do an indefinite integration of a function,
we will get another function.
In other words, the differentiation and integration are mappings from function space to function space.</p>
<p>In scientific computing, usually the problem is to solve some form of differential equations. Consider a general differential equation of the form:</p>
<div class="formula">
ℒ<i>u</i> = <i>f</i>
</div>
<p>where  <span class="formula"><i>u</i></span> and <span class="formula"><i>f</i></span> are some functions on the physical domain, and
<span class="formula">ℒ</span> is some differential operator that maps
the function <span class="formula"><i>u</i></span> to the function <span class="formula"><i>f</i></span>.
Usually, <span class="formula">ℒ</span> and <span class="formula"><i>f</i></span> are given. The task is to solve for <span class="formula"><i>u</i></span>.
That is, we want to learn an operator like the inverse of <span class="formula"><span class="scriptfont">D</span></span> that
maps the function <span class="formula"><i>f</i></span> to the function <span class="formula"><i>u</i></span>.
So the problem of PDE is indeed an operator learning problem.</p>
<p>The classical development of neural networks has been primarily
for mappings between a finite-dimensional Euclidean space and a set of classes
(e.g. an image vector to a label),
or between two finite-dimensional Euclidean spaces (e.g. an image vector to another image vector).
However, many problems in physics and math involve learning the mapping between function spaces,
which poses a limitation on the classical neural network based methods.
Besides all these problem governed by differential equations,
we are learning operators in many common machine learning setting.
For a bold example, images should be considered as functions of light defined on a continuous region,
instead of as <span class="formula">32×32</span> pixel vectors.</p>
<p>In this work, we aim to generalize neural networks so that they can learn operators,
the mappings between infinite-dimensional spaces, with a special focus on PDEs.</p>
</div>
<div class="section" id="limitation-of-fixed-discretization">
<h1>Limitation of Fixed Discretization</h1>
<p>PDEs are, unfortunately, hard.
Instead of learning the operator, people usually discretize the physical domain
and cast the problem in finite-dimensional Euclidean space.
Indeed, hundred years of effort has been made to develop numerical solvers
such as the finite element method and finite difference method.</p>
<img alt="/_static/images/grids.jpg" src="/_static/images/grids.jpg" style="width: 800px;" />
<p>Three examples of discretization.
The left one is a regular grid used in the finite difference method;
the middle one is a triangulated grid used in the finite element method;
the right one is a cylinder mesh for real-world airfoil problem.</p>
<p>Just like how we store images by pixels in <em>.PNG</em> and <em>.JPG</em> formats,
we need to discretize the domain of PDEs into some grid and solve the equation on the grid.
It really makes the thing easier.
These traditional numerical solvers are awesome, but they have some drawbacks:</p>
<ul class="simple">
<li>The error scales steeply with the resolution. We need a high resolution to get good approximations.</li>
<li>The computation and storage steeply scale with the resolution (i.e. the size of the grid).</li>
<li>When the equation is solved on one discretization,</li>
</ul>
<p>we cannot change the discretization anymore.</p>
<p><em>.PNG</em> and <em>.JPG</em> formats are good.
But sometimes maybe we want to save the images as vector images in <em>.EPS</em> or <em>.SVG</em> formats,
so that it can be used and displayed in any context.
And for some images, the vector image format is more convenient and efficient.
Similarly, we want to find the continuous version for PDEs, an operator that is invariant of discretization.</p>
<p>Furthermore, mathematically speaking, such continuous,
discretization-invariant format is in some sense, closer to the real, analytic solution.
It has an important mathematical meaning.
Bear the motivation in mind. Let’s develop a rigorous formulation.</p>
</div>
<div class="section" id="problem-setting">
<h1>Problem Setting</h1>
<p>Let’s be more concrete. Consider the standard second order elliptic PDE</p>
<div class="formula">
 − ∇⋅(<i>a</i>(<i>x</i>)∇<i>u</i>(<i>x</i>)) = <i>f</i>(<i>x</i>),  <i>x</i> ∈ <i>D</i>
</div>
<div class="formula">
<i>u</i>(<i>x</i>) = 0,  <i>x</i> ∈ ∂<i>D</i>
</div>
<p>for some bounded, open domain <span class="formula"><i>D</i> ⊂ ℝ<sup><i>d</i></sup></span> and a fixed source function
<span class="formula"><i>f</i></span>. This equation is prototypical of PDEs arising in
numerous applications including hydrology  and elasticity.
For a given function <span class="formula"><i>a</i></span>,
the equation has a unique weak solution <span class="formula"><i>u</i></span>
and therefore we can define the solution operator <span class="formula">ℱ<sub><i>true</i></sub></span>
as the map from function to function <span class="formula"><i>a</i> ↦ <i>u</i></span>.</p>
<p>Our goal is to learn a operator <span class="formula">ℱ</span> approximating <span class="formula">ℱ<sub><i>true</i></sub></span>,
by using a finite collection of observations of input-output pairs
<span class="formula">{<i>a</i><sub><i>j</i></sub>, <i>u</i><sub><i>j</i></sub>}<span class="scripts"><sup class="script"><i>N</i></sup><sub class="script"><i>j</i> = 1</sub></span></span>, where each <span class="formula"><i>a</i><sub><i>j</i></sub></span> and <span class="formula"><i>u</i><sub><i>j</i></sub></span> are functions on <span class="formula"><i>D</i></span>.
In practice, the training data is solved numerically or observed in experiments.
In other words, functions <span class="formula"><i>a</i><sub><i>j</i></sub></span> and <span class="formula"><i>u</i><sub><i>j</i></sub></span> come with discretization.
Let <span class="formula"><i>P</i><sub><i>K</i></sub> = {<i>x</i><sub>1</sub>, …, <i>x</i><sub><i>K</i></sub>} ⊂ <i>D</i></span> be a <span class="formula"><i>K</i></span>-point discretization of the domain
<span class="formula"><i>D</i></span> and assume we have observations <span class="formula"><i>a</i><sub><i>j</i></sub>|<sub><i>P</i><sub><i>K</i></sub></sub>, <i>u</i><sub><i>j</i></sub>|<sub><i>P</i><sub><i>K</i></sub></sub></span>, for a finite
collection  of input-output pairs indexed by <span class="formula"><i>j</i></span>.
We will show how to learn a discretization-invariant mapping based on discretized data.</p>
</div>
<div class="section" id="kernel-formulation">
<h1>Kernel Formulation</h1>
<p>For a general PDE of the form:</p>
<div class="formula">
(ℒ<sub><i>a</i></sub><i>u</i>)(<i>x</i>) = <i>f</i>(<i>x</i>),  <i>x</i> ∈ <i>D</i>
</div>
<div class="formula">
<i>u</i>(<i>x</i>) = 0,  <i>x</i> ∈ ∂<i>D</i>
</div>
<p>Under fairly general conditions on <span class="formula">ℒ<sub><i>a</i></sub></span>,
we may define the Green’s function <span class="formula"><i>G</i> : <i>D</i>×<i>D</i> → ℝ</span> as the
unique solution to the problem</p>
<div class="formula">
ℒ<sub><i>a</i></sub><i>G</i>(<i>x</i>, ⋅) = <i>δ</i><sub><i>x</i></sub>
</div>
<p>where <span class="formula"><i>δ</i><sub><i>x</i></sub></span> is the delta measure on <span class="formula">ℝ<sup><i>d</i></sup></span> centered at <span class="formula"><i>x</i></span>.
Note that <span class="formula"><i>G</i></span> will depend on the coefficient <span class="formula"><i>a</i></span> thus we will henceforth denote it as <span class="formula"><i>G</i><sub><i>a</i></sub></span>.
Then operator <span class="formula">ℱ<sub><i>true</i></sub></span> can be written as an integral operator of green function:</p>
<div class="formula">
<i>u</i>(<i>x</i>) = <span class="limits"><sup class="limit"> </sup><span class="limit"><span class="bigoperator integral">∫</span></span><sub class="limit"><i>D</i></sub></span><i>G</i><sub><i>a</i></sub>(<i>x</i>, <i>y</i>)<i>f</i>(<i>y</i>) <i>dy</i>
</div>
<p>Generally the Green’s function is continuous at points <span class="formula"><i>x</i> ≠ <i>y</i></span>,
for example, when <span class="formula">ℒ<sub><i>a</i></sub></span> is uniformly elliptic.
Hence it is natural to model the kernel via a neural network <span class="formula"><i>κ</i></span>.
Just as the Green function, the kernel network <span class="formula"><i>κ</i></span> takes input <span class="formula">(<i>x</i>, <i>y</i>)</span>.
Since the kernel depends on <span class="formula"><i>a</i></span>, we let <span class="formula"><i>κ</i></span> also take input <span class="formula">(<i>a</i>(<i>x</i>), <i>a</i>(<i>y</i>))</span>.</p>
<div class="formula">
<i>u</i>(<i>x</i>) = <span class="limits"><sup class="limit"> </sup><span class="limit"><span class="bigoperator integral">∫</span></span><sub class="limit"><i>D</i></sub></span><i>κ</i>(<i>x</i>, <i>y</i>, <i>a</i>(<i>x</i>), <i>a</i>(<i>y</i>))<i>f</i>(<i>y</i>) <i>dy</i>
</div>
</div>
<div class="section" id="as-an-iterative-solver">
<h1>As an Iterative Solver</h1>
<p>In our setting, <span class="formula"><i>f</i></span> is an unknown but fixed function.
Instead of doing the kernel convolution with <span class="formula"><i>f</i></span>,
we will formulate it as an iterative solver
that approximated <span class="formula"><i>u</i></span> by <span class="formula"><i>u</i><sub><i>t</i></sub></span>,
where <span class="formula"><i>t</i> = 0, …, <i>T</i></span> is the time step.</p>
<p>The algorithm starts from an initialization <span class="formula"><i>u</i><sub>0</sub></span>,
for which we use <span class="formula"><i>u</i><sub>0</sub>(<i>x</i>) = (<i>x</i>, <i>a</i>(<i>x</i>))</span>.
At each time step <span class="formula"><i>t</i></span>, it updates <span class="formula"><i>u</i><sub><i>t</i> + 1</sub></span> by an kernel convolution of <span class="formula"><i>u</i><sub><i>t</i></sub></span>.</p>
<div class="formula">
<i>u</i><sub><i>t</i> + 1</sub>(<i>x</i>) = <span class="limits"><sup class="limit"> </sup><span class="limit"><span class="bigoperator integral">∫</span></span><sub class="limit"><i>D</i></sub></span><i>κ</i>(<i>x</i>, <i>y</i>, <i>a</i>(<i>x</i>), <i>a</i>(<i>y</i>))<i>u</i><sub><i>t</i></sub>(<i>x</i>) <i>dy</i>
</div>
<p>It works like an implicit iteration.
At each iteration the algorithm solves an equation of <span class="formula"><i>u</i><sub><i>t</i></sub>(<i>x</i>)</span> and <span class="formula"><i>u</i><sub><i>t</i> + 1</sub>(<i>x</i>)</span>
by the kernel integral. <span class="formula"><i>u</i><sub><i>T</i></sub></span> will be output as the final prediction.</p>
<p>To further take the advantage of neural networks,
we will lift <span class="formula"><i>u</i>(<i>x</i>) ∈ ℝ<sup><i>d</i></sup></span>
to a high dimensional representation <span class="formula"><i>v</i>(<i>x</i>) ∈ ℝ<sup><i>n</i></sup></span>,
with <span class="formula"><i>n</i></span> the dimension of the hidden representation.</p>
<p>The overall algorithmic framework follow:</p>
<div class="formula">
<i>v</i><sub>0</sub>(<i>x</i>) = <i>NN</i><sub>1</sub>(<i>x</i>, <i>a</i>(<i>x</i>))
</div>
<div class="formula">
<i>v</i><sub><i>t</i> + 1</sub>(<i>x</i>) = <i>σ</i><span class="bigdelimiter size2">(</span><i>W</i><i>v</i><sub><i>t</i></sub>(<i>x</i>) + <span class="limits"><sup class="limit"> </sup><span class="limit"><span class="bigoperator integral">∫</span></span><sub class="limit"><i>B</i>(<i>x</i>, <i>r</i>)</sub></span><i>κ</i><sub><i>ϕ</i></sub><span class="bigdelimiter size1">(</span><i>x</i>, <i>y</i>, <i>a</i>(<i>x</i>), <i>a</i>(<i>y</i>)<span class="bigdelimiter size1">)</span><i>v</i><sub><i>t</i></sub>(<i>y</i>) <span class="mathrm">d</span><i>y</i><span class="bigdelimiter size2">)</span> <span class="text">for </span> <i>t</i> = 1, …, <i>T</i>
</div>
<div class="formula">
<i>u</i>(<i>x</i>) = <i>NN</i><sub>2</sub>(<i>v</i><sub><i>T</i></sub>(<i>x</i>))
</div>
<p>where <span class="formula"><i>NN</i><sub>1</sub></span> and <span class="formula"><i>NN</i><sub>2</sub></span> are two feed-forward neural networks
that lifts the initialization to hidden representation <span class="formula"><i>v</i></span>
and projects the representation back to the solution <span class="formula"><i>u</i></span>, respective.
<span class="formula"><i>σ</i></span> is an activation function such as ReLU.
the additional term <span class="formula"><i>W</i> ∈ ℝ<sup><i>n</i>×<i>n</i></sup></span> is a linear transformation
that acts on $v$.
Notice, since the kernel integration happens in the high dimensional representation,
the output of <span class="formula"><i>κ</i><sub><i>ϕ</i></sub></span> is not a scalar,
but a linear transformation <span class="formula"><i>κ</i><sub><i>ϕ</i></sub><span class="bigdelimiter size1">(</span><i>x</i>, <i>y</i>, <i>a</i>(<i>x</i>), <i>a</i>(<i>y</i>)<span class="bigdelimiter size1">)</span> ∈ ℝ<sup><i>n</i>×<i>n</i></sup></span>.</p>
</div>
<div class="section" id="graph-neural-networks">
<h1>Graph Neural Networks</h1>
<p>To do the integration, we again need some discretization.
Assuming a uniform distribution of <span class="formula"><i>y</i></span>,
the integral <span class="formula"><span class="limits"><span class="limit"><span class="bigoperator integral">∫</span></span></span><sub><i>B</i>(<i>x</i>, <i>r</i>)</sub><i>κ</i><sub><i>ϕ</i></sub><span class="bigdelimiter size1">(</span><i>x</i>, <i>y</i>, <i>a</i>(<i>x</i>), <i>a</i>(<i>y</i>)<span class="bigdelimiter size1">)</span><i>v</i><sub><i>t</i></sub>(<i>y</i>) <span class="mathrm">d</span><i>y</i></span> can be approximated by a sum:</p>
<div class="formula">
<span class="fraction"><span class="ignored">(</span><span class="numerator">1</span><span class="ignored">)/(</span><span class="denominator">|<i>N</i>|</span><span class="ignored">)</span></span><span class="limits"><sup class="limit"> </sup><span class="limit"><span class="bigoperator">∑</span></span><sub class="limit"><i>y</i> ∈ <i>N</i>(<i>x</i>)</sub></span><i>κ</i>(<i>x</i>, <i>y</i>, <i>a</i>(<i>x</i>), <i>a</i>(<i>y</i>))<i>v</i><sub><i>t</i></sub>(<i>y</i>)
</div>
<p>Observation: the kernel integral is equivalent to the message passing on graphs</p>
<p>If you are similar with graph neural network,
you may have already realized this formulation is the same as
the aggregation of messages in graph network.
Message passing graph networks comprise a standard architecture employing edge features
(gilmer et al, 2017).</p>
<p>If we properly construct graphs on the spatial domain <span class="formula"><i>D</i></span> of the PDE,
the kernel integration can be viewed as an aggregation of messages.
Given node features <span class="formula"><i>v</i><sub><i>t</i></sub>(<i>x</i>) ∈ ℝ<sup><i>n</i></sup></span>,
edge features <span class="formula"><i>e</i>(<i>x</i>, <i>y</i>) ∈ ℝ<sup><i>n</i><sub><i>e</i></sub></sup></span>,
and a graph <span class="formula"><i>G</i></span>, the message passing neural network with averaging aggregation is</p>
<div class="formula">
<i>v</i><sub><i>t</i> + 1</sub>(<i>x</i>) = <i>σ</i><span class="bigdelimiter size2">(</span><i>W</i><i>v</i><sub><i>t</i></sub>(<i>x</i>) + <span class="fraction"><span class="ignored">(</span><span class="numerator">1</span><span class="ignored">)/(</span><span class="denominator">|<i>N</i>(<i>x</i>)|</span><span class="ignored">)</span></span><span class="limits"><sup class="limit"> </sup><span class="limit"><span class="bigoperator">∑</span></span><sub class="limit"><i>y</i> ∈ <i>N</i>(<i>x</i>)</sub></span><i>κ</i><sub><i>ϕ</i></sub><span class="bigdelimiter size1">(</span><i>e</i>(<i>x</i>, <i>y</i>)<span class="bigdelimiter size1">)</span><i>v</i><sub><i>t</i></sub>(<i>y</i>)<span class="bigdelimiter size2">)</span>
</div>
<p>where <span class="formula"><i>W</i> ∈ ℝ<sup><i>n</i>×<i>n</i></sup></span>,
<span class="formula"><i>N</i>(<i>x</i>)</span> is the neighborhood of <span class="formula"><i>x</i></span> according to the graph,
<span class="formula"><i>κ</i><sub><i>ϕ</i></sub><span class="bigdelimiter size1">(</span><i>e</i>(<i>x</i>, <i>y</i>)<span class="bigdelimiter size1">)</span></span> is a neural network
taking as input edge features and as output
a matrix in <span class="formula">ℝ<sup><i>n</i>×<i>n</i></sup></span>.
Relating to our kernel formulation, <span class="formula"><i>e</i>(<i>x</i>, <i>y</i>) = (<i>x</i>, <i>y</i>, <i>a</i>(<i>x</i>), <i>a</i>(<i>y</i>))</span>.</p>
<img alt="/_static/images/graph.jpg" src="/_static/images/graph.jpg" style="width: 800px;" />
</div>
<div class="section" id="nystrom-approximation">
<h1>Nystrom Approximation</h1>
<p>Ideally, to use all the information available,
we should construct <span class="formula"><i>K</i></span> nodes in the graph for all the points in the discretization
<span class="formula"><i>P</i><sub><i>k</i></sub> = {<i>x</i><sub>1</sub>, …, <i>x</i><sub><i>K</i></sub>}</span>, which will create <span class="formula"><i>O</i>(<i>K</i><sup>2</sup>)</span> edges.
It is quite expensive.
Thankfully, we don’t need all the points to get an accurate approximation.
For each graph, the error of Monte Carlo approximation of the kernel integral
<span class="formula"><span class="limits"><span class="limit"><span class="bigoperator integral">∫</span></span></span><sub><i>B</i>(<i>x</i>, <i>r</i>)</sub><i>κ</i><sub><i>ϕ</i></sub>(<i>x</i>, <i>y</i>)<i>v</i><sub><i>t</i></sub>(<i>y</i>) <span class="mathrm">d</span><i>y</i></span> scales with <span class="formula"><i>m</i><sup> − 1 ⁄ 2</sup></span>,
where <span class="formula"><i>m</i></span> is the number of nodes sampled.</p>
<p>Since we will sample <span class="formula"><i>N</i></span> graphs in total for all <span class="formula"><i>N</i></span> training examples <span class="formula">{<i>a</i><sub><i>j</i></sub>, <i>u</i><sub><i>j</i></sub>}<sup><i>N</i></sup></span>,
the overall error of the kernel is much smaller than <span class="formula"><i>m</i><sup> − 1 ⁄ 2</sup></span>.
In practice, sampling <span class="formula"><i>m</i> ∼ 200</span> nodes is sufficient for <span class="formula"><i>K</i> ∼ 100000</span> points.</p>
<p>It is possible to further improve the approximation
using more sophisticated Nystrom Approximation methods.
For example, we can estimate the importance of each points,
and add more nodes to the difficult and singularity areas in the PDEs.</p>
</div>
<div class="section" id="experiments-poisson-equations">
<h1>Experiments: Poisson Equations</h1>
<p>First let’s do a sanity check. Consider a simple poisson equation:</p>
<div class="formula">
 − Δ<i>u</i> = <i>f</i>
</div>
<p>We set <span class="formula"><i>v</i><sub>0</sub> = <i>f</i></span> and <span class="formula"><i>T</i> = 1</span>, using one iteration of the graph kernel network
to learn the operator <span class="formula">ℱ : <i>f</i> ↦ <i>u</i></span>.</p>
<div class="section" id="poisson-equation">
<h2>poisson equation</h2>
<img alt="/_static/images/nik_kernel.jpg" src="/_static/images/nik_kernel.jpg" style="width: 800px;" />
<p>As shown in the figure above, we compare the true analytic Green function <span class="formula"><i>G</i>(<i>x</i>, <i>y</i>)</span> (left)
with the learned kernel <span class="formula"><i>κ</i><sub><i>ϕ</i></sub>(<i>x</i>, <i>y</i>)</span>  (right).
The learned kernel is almost the same as the true kernel,
which means are neural network formulation does match the Green function expression.</p>
</div>
<div class="section" id="d-poisson-equation">
<h2>2D poisson equation</h2>
<img alt="/_static/images/GKN_compare.jpg" src="/_static/images/GKN_compare.jpg" style="width: 800px;" />
<p>By assuming the kernel structure,
graph kernel networks need only a few training examples to learn the shape of solution <span class="formula"><i>u</i></span>.
As shown in the figure above, the graph kernel network can roughly learn <span class="formula"><i>u</i></span> with <span class="formula">5</span> training pairs,
which a feedforward neural network need at least <span class="formula">100</span> training examples.</p>
</div>
</div>
<div class="section" id="experiments-generalization-of-resolution">
<h1>Experiments: generalization of resolution</h1>
<p>For the large scale experiments, we use Darcy equation of the form</p>
<div class="formula">
 − ∇⋅(<i>a</i>(<i>x</i>)∇<i>u</i>(<i>x</i>)) = <i>f</i>(<i>x</i>),  <i>x</i> ∈ <i>D</i>
</div>
<div class="formula">
<i>u</i>(<i>x</i>) = 0,  <i>x</i> ∈ ∂<i>D</i>
</div>
<p>and learn the operator <span class="formula">ℱ : <i>a</i> ↦ <i>u</i></span>.</p>
<p>To show the generalization property, we train the graph kernel network
with nodes sampled from the resolution <span class="formula"><i>s</i>×<i>s</i></span>
and test on another resolution <span class="formula"><i>s</i>’×<i>s</i>’</span> .</p>
<p>As shown in the table above for each row,
the test errors on different resolutions are about the same,
which means the graph kernel network can
also generalize in the semi-supervised setting.
An figure for <span class="formula"><i>s</i> = 16, <i>s</i>’ = 241</span> is following (where error is absolute squared error):</p>
<img alt="/_static/images/uai_16to241.jpg" src="/_static/images/uai_16to241.jpg" style="width: 800px;" />
</div>
<div class="section" id="conclusion">
<h1>Conclusion</h1>
<p>In the work we purposed to use graph networks for operator learning in PDE problems.
By varying the underlying graph and discretization,
the learned kernel is invariant of the discretization.
Experiments confirm the graph kernel networks are able to generalize among different discretization.
And in the fixed discretization setting, the graph kernel networks also have good performances compared to several benchmark.</p>
</div>
<div class="section" id="references">
<h1>References</h1>
<table class="docutils footnote" frame="void" id="footnote-1" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-1">[1]</a></td><td>Neural operator: Graph kernel network for partial differential equations,
Zongyi Li, Nikola Kovachki, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, Anima Anandkumar</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-2" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-2">[2]</a></td><td>Neural operator: Learning maps between function spaces,
Nikola Kovachki, Zongyi Li, Kamyar Azizzadenesheli, Burigede Liu, Kaushik Bhattacharya, Andrew Stuart, Anima Anandkumar</td></tr>
</tbody>
</table>
<table class="docutils footnote" frame="void" id="footnote-3" rules="none">
<colgroup><col class="label" /><col /></colgroup>
<tbody valign="top">
<tr><td class="label"><a class="fn-backref" href="#footnote-reference-3">[3]</a></td><td>Blog post by Zongyi Li, <a class="reference external" href="https://zongyi-li.github.io/blog/2020/graph-pde/">https://zongyi-li.github.io/blog/2020/graph-pde/</a></td></tr>
</tbody>
</table>
</div>
</div>
</body>
</html>
